<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MIX: A Multi-view Time-Frequency Interactive Explanation Framework for Time Series Classification</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
        }
        h1, h2, h3 {
            line-height: 1.3;
        }
        strong {
            font-weight: 600;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-12 max-w-5xl">
        <!-- Header Section -->
        <header class="text-center mb-10">
            <h1 class="text-3xl md:text-4xl font-bold text-gray-900">MIX: A Multi-view Time-Frequency Interactive Explanation Framework for Time Series Classification</h1>
            <p class="text-md text-gray-600 mt-4">NeurIPS 2025</p>
        </header>

        <!-- Authors Section -->
        <section class="text-center mb-10">
            <div class="text-lg">
                <p>Viet-Hung Tran<sup>(*1)</sup> | Ngoc Phu Doan<sup>(*1)</sup> | Zichi Zhang<sup>(*1)</sup></p>
                <p class="mt-2">Tuan Dung Pham<sup>(1)</sup> | Phi Hung Nguyen<sup>(1)</sup> | Xuan Hoang Nguyen<sup>(2)</sup> | Hans Vandierendonck<sup>(1)</sup> | Ira Assent<sup>(3)</sup>|  Thai Son Mai<sup>(1)</sup></p>
            </div>
            <div class="text-md text-gray-600 mt-4">
                <p><sup>(1)</sup> Queen's University Belfast | <sup>(2)</sup> Institut Polytechnique de Paris</p>
                <p><sup>(3)</sup>Aarhus University </p>
            </div>
            <div class="text-sm text-gray-500 mt-4">
                <p><sup>(*)</sup> The first three authors contributed equally</p>
            </div>
        </section>
        
        <!-- Links Section -->
        <section class="text-center mb-12">
            <div class="flex justify-center gap-x-6">
                <a href="https://openreview.net/pdf?id=XDtwXau0BX" class="text-lg font-medium text-blue-600 hover:underline">Paper</a>
                <a href="#" class="text-lg font-medium text-blue-600 hover:underline">Github</a>
                <a href="#" class="text-lg font-medium text-blue-600 hover:underline">Models</a>
            </div>
        </section>


        <!-- Abstract Section -->
        <section class="mb-12">
            <h2 class="text-2xl font-bold text-center mb-6">Abstract</h2>
            <div class="bg-white p-8 rounded-lg shadow-md">
                <!-- <p> -->
                    <!-- MIX: A Multi-view Time-Frequency Interactive Explanation Framework for Time Series Classification -->
                <!-- </p> -->
                <p class="mt-4">
                   Deep learning models for time series classification (TSC) have achieved impressive performance, but explaining their decisions remains a significant challenge. Existing post-hoc explanation methods typically operate solely in the time domain and from a single-view perspective, limiting both faithfulness and robustness. In this work, we propose MIX (Multi-view Time-Frequency Interactive EXplanation Framework), a novel framework that helps to explain deep learning models in a multi-view setting by leveraging multi-resolution, time-frequency views constructed using the Haar Discrete Wavelet Transform (DWT). MIX introduces an <em>interactive</em> cross-view refinement scheme, where explanation's information from one view is propagated across views to enhance overall interpretability. To align with user-preferred perspectives, we propose a greedy selection strategy that traverses the multi-view space to identify the most informative features. Additionally, we present OSIGV, a user-aligned segment-level attribution mechanism based on overlapping windows for each view, and introduce keystone-first IG, a method that refines explanations in each view using additional information from another view. Extensive experiments across multiple TSC benchmarks and model architectures demonstrate that MIX significantly outperforms state-of-the-art (SOTA) methods in terms of explanation faithfulness and robustness.
                </p>
            </div>
        </section>

        <!-- Figures Section -->
        <section class="mb-12">
            <!-- Figure 1 -->
            <div class="bg-white p-8 rounded-lg shadow-md mb-8">
                 <div class="flex justify-center mb-4">
                    <img src="https://github.com/hung-z/MIX/blob/main/fig_pipeline_new.png" alt="Diagram of the MIX Framework" class="rounded-md w-full max-w-3xl">
                 </div>
                <p class="text-center text-gray-700 italic">
                   <p>Overview of the MIX framework with three phases.</p>

                    <p><strong>(A)</strong> <em>Multi-view Construction and Independent Explanation</em> described in Section~subset:Overview_Algorithm: views V<sub>r</sub> are constructed via Haar DWT, then explained independently using IGV and OSIGV.</p>

                    <p><strong>(B)</strong> <em>Cross-view Refinement</em> described in Section~subset:Overview_Algorithm: the best view V<sub>q</sub> is selected using KAUCS̃<sub>top</sub>, then refined using KIGV and OSIGV guided by top-h segments.</p>

                    <p><strong>(C)</strong> <em>Multi-view Greedy Selection</em> described in Section~subset:Overview_Algorithm: MIX traverses all views to select key features and maps them to the user-preferred view. Phase 3 is practical for selecting top features directly.</p>

                    <p><strong>(D)</strong> Attribution mechanism in Phase 1: IGV is applied to each view, and scores are aggregated into overlapping segments via OSIGV (see Section~subsect:attribution_MIX).</p>

                    <p><strong>(E)</strong> Attribution mechanism in Phase 2: Keystone-first IG for view (KIGV) is used to prioritize keystone features before generate importance score to others (see Section~subsect:attribution_MIX), then apply OSIGV again to overlapping segments.</p>

                    <!-- <p>Refer Section sec:app:table_of_symbols and Table tab:notation for meaning of symbols.</p> -->
                </p>
            </div>

            <!-- Figure 2 -->
            <div class="bg-white p-8 rounded-lg shadow-md">
                 <div class="flex justify-center mb-4">
                     <img src="https://placehold.co/800x400/e2e8f0/64748b?text=Figure+2:+Feature+Space+Analysis" alt="Diagram of the feature space analysis" class="rounded-md w-full max-w-3xl">
                 </div>
                <p class="text-center text-gray-700 italic">
                    <strong>Figure 2: Feature space analysis to show that, under MIDRE, <em>f<sub>recon</sub><sup>MIDRE</sup></em> and <em>f<sub>priv</sub><sup>MIDRE</sup></em> have a discrepancy, degrading MI attack.</strong> We visualize penultimate layer activations of private images (★ <em>f<sub>priv</sub></em>), RE-private images (▼ <em>f<sub>RE</sub></em>), and MI-reconstructed images (× <em>f<sub>recon</sub></em>) generated by both (a) NoDef and (b) our MIDRE model. We also visualize the convex hull for private images, RE-private images, and MI-reconstructed images. In (a), <em>f<sub>recon</sub><sup>NoDef</sup></em> closely resembles <em>f<sub>priv</sub><sup>NoDef</sup></em>, consistent with high attack accuracy. In (b), private images and RE-private images share some similarity but they are not identical, with partial overlap between <em>f<sub>priv</sub><sup>MIDRE</sup></em> and <em>f<sub>RE</sub><sup>MIDRE</sup></em>. Importantly, <em>f<sub>recon</sub><sup>MIDRE</sup></em> closely resembles <em>f<sub>RE</sub><sup>MIDRE</sup></em> as RE-private is the training data for MIDRE. This results in <strong>a reduced overlap between <em>f<sub>recon</sub><sup>MIDRE</sup></em> and <em>f<sub>priv</sub><sup>MIDRE</sup></em>, explaining that MI does not accurately capture the private image features under MIDRE.</strong> More visualization can be found in Supp.
                </p>
            </div>
        </section>

        <!-- Citation -->
        <section class="mb-12">
            <h2 class="text-2xl font-bold text-center mb-6">Citation</h2>
            <div class="bg-gray-900 text-gray-200 p-6 rounded-lg shadow-md font-mono text-sm overflow-x-auto">
                <pre><code>@article{
    tran2025random,
    title={Random Erasing vs. Model Inversion: A Promising Defense or a False Hope?},
    author={Viet-Hung Tran and Ngoc-Bao Nguyen and Son T. Mai and Hans Vandierendonck and Ira Assent and Alex Kot and Ngai-Man Cheung},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2025},
    url={https://openreview.net/forum?id=S9CwKnPHaO},
    note={Featured Certification}
}</code></pre>
            </div>
        </section>
        
        <!-- Acknowledgements -->
        <section>
             <h2 class="text-2xl font-bold text-center mb-6">Acknowledgements</h2>
             <div class="bg-white p-8 rounded-lg shadow-md text-gray-600 text-sm">
                <p>This research is supported by the National Research Foundation, Singapore under its AI Singapore Programmes (AISG Award No.: AISG2-TC-2022-007); The Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021). This research is supported by the National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Infocomm Media Development Authority. This research is also part-funded by the European Union (Horizon Europe 2021-2027 Framework Programme Grant Agreement number 10107245. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. The European Union cannot be held responsible for them) and by the Engineering and Physical Sciences Research Council under grant number EP/X029174/1.</p>
             </div>
        </section>

    </div>

</body>
</html>

